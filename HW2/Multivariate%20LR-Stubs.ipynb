{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import or define evaluate_classification, plot_decision_boundary, plot_data\n",
        "\n",
        "from sklearn.datasets import make_classification, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filename, data_columns, target_column):\n",
        "    \"\"\"Load dataset from CSV file.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to CSV file.\n",
        "        data_columns (list): List of column names for data.\n",
        "        target_column (str): Name of target column.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing data and target.\n",
        "    \"\"\"\n",
        "    \n",
        "    df = pd.read_csv(filename)\n",
        "    X, y = df[data_columns], df[target_column]\n",
        "    return X.values, y.values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"Logistic Regression Classifier.\"\"\"\n",
        "    def __init__(self, standardize=True, \n",
        "                 learning_rate=0.01, \n",
        "                 max_iter=1000,\n",
        "                 tol=1e-4,\n",
        "                 verbose=False):\n",
        "        \"\"\"Initialize Logistic Regression Classifier.\n",
        "        \n",
        "        Args:\n",
        "            standardize (bool): Whether to standardize the data.\n",
        "            learning_rate (float): Learning rate for gradient descent.\n",
        "            max_iter (int): Maximum number of iterations for gradient descent.\n",
        "            tol (float): Tolerance for gradient descent.\n",
        "            verbose (bool): Whether to print cost at each 100th iteration.\n",
        "        \"\"\"\n",
        "        self.standardize = standardize\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def normalize(self, X):\n",
        "        \"\"\"Normalize the data.\n",
        "        \n",
        "        Args:\n",
        "            X (array): Data to normalize.\n",
        "        \n",
        "        Returns:\n",
        "            tuple: Tuple containing normalized data, mean, and standard deviation.\n",
        "        \"\"\"\n",
        "        # TODO: Implement\n",
        "        mean = None\n",
        "        std = None\n",
        "        X_new = None\n",
        "        return X_new, mean, std\n",
        "\n",
        "    def add_intercept(self, X):\n",
        "        \"\"\"Add intercept term to the data.\n",
        "        \n",
        "        Args:\n",
        "            X (array): Data to add intercept term.\n",
        "        \n",
        "        Returns:\n",
        "            array: Data with intercept term.\"\"\"\n",
        "        m = X.shape[0]\n",
        "        ones = np.ones((m, 1))\n",
        "        X_new = np.column_stack((ones, X))\n",
        "        return X_new\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Sigmoid function.\n",
        "        \n",
        "        Args:\n",
        "            z (array): Input to sigmoid function.\n",
        "            \n",
        "        Returns:\n",
        "            array: Output of sigmoid function.\"\"\"\n",
        "        # TODO: Implement\n",
        "        h = None\n",
        "        return h\n",
        "    \n",
        "    def hypothesis(self, X, theta):\n",
        "        \"\"\"Hypothesis function.\n",
        "        \n",
        "        Args:\n",
        "            X (array): Data.\n",
        "            theta (array): Parameters.\n",
        "        \n",
        "        Returns:\n",
        "            array: Output of hypothesis function.\"\"\"\n",
        "        # TODO: Implement\n",
        "        z = None\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def cost_function(self, X, y, theta):\n",
        "        \"\"\"Cost function.\n",
        "        \n",
        "        Args:\n",
        "            X (array): Data.\n",
        "            y (array): Target.\n",
        "            theta (array): Parameters.\n",
        "            \n",
        "        Returns:\n",
        "            float: Cost of hypothesis function.\"\"\"\n",
        "        # TODO: Implement\n",
        "        cost = None\n",
        "        return cost\n",
        "\n",
        "    def gradient(self, X, y, theta):\n",
        "        \"\"\"Gradient of cost function.\n",
        "        \n",
        "        Args:\n",
        "            X (array): Data.\n",
        "            y (array): Target.\n",
        "            theta (array): Parameters.\n",
        "            \n",
        "        Returns:\n",
        "            array: Gradient of cost function.\"\"\"\n",
        "        # TODO: Implement\n",
        "        grad = None\n",
        "        return grad\n",
        "\n",
        "    def gradient_descent(self, X, y, theta):\n",
        "        \"\"\"Gradient descent algorithm.\n",
        "\n",
        "        Args:\n",
        "            X (array): Data.\n",
        "            y (array): Target.\n",
        "            theta (array): Parameters.\n",
        "        \n",
        "        Returns:\n",
        "            tuple: Tuple containing parameters and costs.\"\"\"\n",
        "        costs = []\n",
        "        J = self.cost_function(X, y, theta)\n",
        "        costs.append(J)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Iteration 0 Cost: {J}\")\n",
        "\n",
        "        for i in range(1, self.max_iter + 1):\n",
        "            # TODO: Implement\n",
        "            grad = None\n",
        "            theta = None\n",
        "            cost = None\n",
        "            \n",
        "            costs.append(cost)\n",
        "\n",
        "            if i % 100 == 0 and self.verbose:\n",
        "                print(f\"Iteration {i} Cost: {cost}\")\n",
        "\n",
        "            if np.abs(costs[i] - costs[i - 1]) < self.tol:\n",
        "                print(f\"Converged at iteration {i}\")\n",
        "                break\n",
        "\n",
        "        return theta, costs\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the model.\n",
        "\n",
        "        Args:\n",
        "            X (array): Data.\n",
        "            y (array): Target.\"\"\"\n",
        "        X_new = X.copy()\n",
        "        if self.standardize:\n",
        "            X_new, self.mean, self.std = self.normalize(X_new)\n",
        "        X_new = self.add_intercept(X_new)\n",
        "\n",
        "        self.theta = np.zeros(X_new.shape[1])\n",
        "        self.theta, self.costs = self.gradient_descent(X_new, y, self.theta)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the target.\n",
        "\n",
        "        Args:\n",
        "            X (array): Data.\n",
        "\n",
        "        Returns:\n",
        "            array: Predicted target.\"\"\"\n",
        "        X_new = X.copy()\n",
        "        # TODO: Implement\n",
        "        if self.standardize:\n",
        "            X_new = None\n",
        "        X_new = None\n",
        "\n",
        "        y_pred = None\n",
        "        return y_pred\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict the probability of target.\n",
        "        \n",
        "        Args:\n",
        "            X (array): Data.\n",
        "        \n",
        "        Returns:\n",
        "            array: Predicted probability of target.\"\"\"\n",
        "        X_new = X.copy()\n",
        "        # TODO: Implement\n",
        "        if self.standardize:\n",
        "            X_new = None\n",
        "        X_new = None\n",
        "\n",
        "        h = None\n",
        "        return np.column_stack((1-h, h))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples = 200, n_classes = 2, n_features = 2, \n",
        "                           n_informative=2, n_redundant=0, random_state = 42,\n",
        "                           flip_y=0.02, class_sep=0.8)\n",
        "plot_data(X, y)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(standardize=True, learning_rate=0.01, max_iter=1000, tol=1e-4, verbose=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lr.fit(X_train, y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = lr.predict(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
        "print(f\"Accuracy: \\n{accuracy}\")\n",
        "print(f\"Report: \\n{report}\")\n",
        "print(f\"Confusion: \\n{confusion}\")\n",
        "plot_decision_boundary(lr, X_test, y_test)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "plot_data(X, y)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "lr = LogisticRegression(standardize=True, learning_rate=0.01, max_iter=1000, tol=1e-4, verbose=True)\n",
        "lr.fit(X_train, y_train)\n",
        "y_test_pred = lr.predict(X_test)\n",
        "\n",
        "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
        "print(f\"Accuracy: \\n{accuracy}\")\n",
        "print(f\"Report: \\n{report}\")\n",
        "print(f\"Confusion: \\n{confusion}\")\n",
        "plot_decision_boundary(lr, X_test, y_test)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Evaluate the above LogisticRegression class on datasets sats.csv and tests.csv. Consider using polynomial features when applicable."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_columns = [\"exam1\", \"exam2\"]\n",
        "target_column = \"submitted\"\n",
        "X, y = load_dataset('../../Data/Classification/sats.csv', data_columns, target_column)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(standardize=True, learning_rate=0.01, max_iter=1000, tol=1e-4, verbose=True)\n",
        "lr.fit(X_train, y_train)\n",
        "y_test_pred = lr.predict(X_test)\n",
        "\n",
        "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
        "print(f\"Accuracy: \\n{accuracy}\")\n",
        "print(f\"Report: \\n{report}\")\n",
        "print(f\"Confusion: \\n{confusion}\")\n",
        "plot_decision_boundary(lr, X_test, y_test)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "da1350d706778574eb7298bca1bd7718ec03806ad26d0510a11ed17a26830b5f"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}